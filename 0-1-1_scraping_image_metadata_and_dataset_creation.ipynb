{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image metadata scraping and CSV dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image metadata obtainment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## path thingy\n",
    "try: #scripts\n",
    "    current_dir = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # jupyter\n",
    "    current_dir = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define the input timestamp in ISO 8601 format\n",
    "input_timestamp = \"2024-09-30T06:25:02.809Z\"  # Replace with your desired timestamp\n",
    "\n",
    "# Function to convert an ISO 8601 date string to a Unix timestamp in milliseconds with a 2-hour offset\n",
    "def iso_to_timestamp(iso_str):\n",
    "    # Parse the ISO 8601 date string (including milliseconds and 'Z' indicating UTC)\n",
    "    date = datetime.strptime(iso_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    # Add 2 hours to the parsed date\n",
    "    adjusted_date = date + timedelta(hours=2)\n",
    "    # Convert to Unix timestamp (in milliseconds)\n",
    "    timestamp = int(adjusted_date.timestamp() * 1000)\n",
    "    return timestamp\n",
    "\n",
    "# Convert the input timestamp to an initial cursor\n",
    "initial_cursor = iso_to_timestamp(input_timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_images = 2000000000 # some huge number now redundant because API limit at 50'000\n",
    "\n",
    "def get_image_metadata():\n",
    "    base_url = \"https://civitai.com/api/v1/images\"\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer APITOKEN\"  # Replace with your actual API token\n",
    "    }\n",
    "    params = {\n",
    "        \"sort\": \"Most Reactions\",\n",
    "        \"nsfw\": \"Soft\",\n",
    "        \"cursor\": f\"0|{initial_cursor}\"\n",
    "    }\n",
    "\n",
    "    # Use pathlib to create the base directory for saving files\n",
    "    current_dir = Path.cwd()\n",
    "    base_directory_path = current_dir / f\"data/raw/001/{input_timestamp.replace(':', '').replace('T', '_').replace('.', '_').replace('Z', '')}\"\n",
    "    file_counter = 0\n",
    "\n",
    "    # Create a folder based on the input timestamp\n",
    "    base_directory_path.mkdir(parents=True, exist_ok=True)\n",
    "    sub_directory_path = base_directory_path\n",
    "\n",
    "    retry_delay = 300  # 300 seconds / 5 minutes\n",
    "    retries_without_cursor = 0  # Track consecutive retries without a new cursor\n",
    "\n",
    "    while True:\n",
    "        response = requests.get(base_url, headers=headers, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            items = data.get('items', [])\n",
    "            if not items:\n",
    "                print(\"No more data available.\")\n",
    "                retries_without_cursor += 1\n",
    "                if retries_without_cursor > 5:  # Allow up to 5 retries before stopping\n",
    "                    print(\"Reached the end of the data after multiple retries.\")\n",
    "                    break\n",
    "                time.sleep(retry_delay)\n",
    "                continue\n",
    "\n",
    "            retries_without_cursor = 0  # Reset if we get data\n",
    "\n",
    "            next_cursor = data.get('metadata', {}).get('nextCursor')\n",
    "            if next_cursor:\n",
    "                # Increment the cursor by 50 (e.g., \"0|1722470401000\" -> \"50|1722470401000\")\n",
    "                cursor_value = int(params['cursor'].split(\"|\")[0])\n",
    "                new_cursor_value = cursor_value + 50\n",
    "                params['cursor'] = f\"{new_cursor_value}|{params['cursor'].split('|')[1]}\"\n",
    "            else:\n",
    "                print(\"No new cursor returned, stopping.\")\n",
    "                break\n",
    "\n",
    "            file_counter += 1\n",
    "            if file_counter % max_images == 0:\n",
    "                time_stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                sub_directory_path = base_directory_path / f\"{input_timestamp.replace(':', '').replace('T', '_').replace('.', '_').replace('Z', '')}_session_{time_stamp}\"\n",
    "                sub_directory_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            file_path = sub_directory_path / f'most_recent_{file_counter}.json'\n",
    "            with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                json.dump(data, file, indent=4)\n",
    "\n",
    "        elif response.status_code == 502:\n",
    "            print(f\"Received HTTP 502. Retrying in {retry_delay // 60} minutes.\")\n",
    "            time.sleep(retry_delay)  # Wait for 5 minutes before retrying\n",
    "        else:\n",
    "            print(f\"Failed to fetch data: HTTP {response.status_code}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_image_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m, in \u001b[0;36mget_image_metadata\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m retries_without_cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Track consecutive retries without a new cursor\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     30\u001b[0m         data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/anaconda3/envs/latm/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/latm/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/latm/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/latm/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/latm/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/envs/latm/lib/python3.10/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/latm/lib/python3.10/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/latm/lib/python3.10/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/anaconda3/envs/latm/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/latm/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/latm/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/latm/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/latm/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/envs/latm/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_image_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## json chronological data sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_files(source_dir, target_dir, max_items_per_file=100):\n",
    "    print(f\"Starting to organize files from {source_dir} to {target_dir}\")\n",
    "    item_buffer = []\n",
    "    file_count = 0\n",
    "\n",
    "    # Walk through all files in the source directory\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        if '.ipynb_checkpoints' in root:\n",
    "            continue  # Skip .ipynb_checkpoints directories\n",
    "        print(f\"Checking directory: {root}\")\n",
    "        for filename in files:\n",
    "            #print(f\"Found files: {files}\")\n",
    "            if filename.lower().endswith('.json'):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                try:\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        data = json.load(file)\n",
    "                        items = data.get('items', [])  # Get the list of items\n",
    "                        for item in items:\n",
    "                            item_buffer.append(item)\n",
    "                            # Write out the buffer if it has reached the maximum size\n",
    "                            if len(item_buffer) >= max_items_per_file:\n",
    "                                write_items(item_buffer[:max_items_per_file], target_dir)\n",
    "                                item_buffer = item_buffer[max_items_per_file:]\n",
    "                                file_count += 1\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Error decoding JSON from file {file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred with file {file_path}: {e}\")\n",
    "\n",
    "    # Write any remaining items in the buffer\n",
    "    if item_buffer:\n",
    "        write_items(item_buffer, target_dir)\n",
    "        file_count += 1\n",
    "\n",
    "    #print(f\"Processed {file_count} files.\")\n",
    "\n",
    "def write_items(items, target_dir):\n",
    "    # Use the createdAt from the first item to determine the directory\n",
    "    created_at = items[0].get('createdAt')\n",
    "    if created_at:\n",
    "        date_obj = datetime.fromisoformat(created_at.rstrip(\"Z\"))\n",
    "        new_dir = os.path.join(target_dir, f\"{date_obj.year}\", f\"{date_obj.year}-{date_obj.month:02d}\", f\"{date_obj.year}-{date_obj.month:02d}-{date_obj.day:02d}\")\n",
    "        os.makedirs(new_dir, exist_ok=True)\n",
    "        new_file_path = os.path.join(new_dir, f\"batch_{date_obj.strftime('%Y%m%dT%H%M%S')}.json\")\n",
    "        with open(new_file_path, 'w', encoding='utf-8') as new_file:\n",
    "            json.dump(items, new_file, indent=4)\n",
    "        #print(f\"Wrote {len(items)} items to {new_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to organize files from /home/lauhp/000_PHD/000_005_COURSES/SD-Social/data/raw/001 to /home/lauhp/000_PHD/000_005_COURSES/SD-Social/data/image-metadata/json-sorted\n",
      "Checking directory: /home/lauhp/000_PHD/000_005_COURSES/SD-Social/data/raw/001\n",
      "Checking directory: /home/lauhp/000_PHD/000_005_COURSES/SD-Social/data/raw/001/2024-09-30_062502_809\n"
     ]
    }
   ],
   "source": [
    "source_dir = current_dir / 'data/raw/001'\n",
    "target_dir = current_dir / 'data/image-metadata/json-sorted'\n",
    "\n",
    "organize_files(source_dir, target_dir, max_items_per_file=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV creation and user name hashingÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### walk through json files and write to csv functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: #scripts\n",
    "    current_dir = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # jupyter\n",
    "    current_dir = Path.cwd()\n",
    "\n",
    "def find_json_files(directory):\n",
    "    \"\"\"Walk through the directory and its subdirectories to find all JSON files.\"\"\"\n",
    "    json_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                json_files.append(os.path.join(root, file))\n",
    "    return json_files\n",
    "\n",
    "def write_to_csv(json_files, output_csv):\n",
    "    \"\"\"Read JSON files, extract data, and write to a CSV file.\"\"\"\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = [\n",
    "            'createdAt', 'url', 'positivePrompt', 'negativePrompt', 'nsfw', 'nsfwLevel',\n",
    "            'browsingLevel', 'likeCount', 'dislikeCount', 'heartCount', 'cryCount', \n",
    "            'laughCount', 'commentCount', 'username', 'Model', 'Meta', 'VAE'\n",
    "        ] + [f'Resource{i+1}' for i in range(6)]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for json_file in json_files:\n",
    "            with open(json_file, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                for item in data:\n",
    "                    meta = item.get('meta', {}) or {}\n",
    "                    stats = item.get('stats', {}) or {}\n",
    "                    resources = meta.get('resources', []) if isinstance(meta, dict) else []\n",
    "\n",
    "                    # Simplified Meta details without VAE\n",
    "                    meta_details = (\n",
    "                        f\"Size: {meta.get('Size', '')}, Seed: {meta.get('seed', '')}, Steps: {meta.get('steps', '')}, \"\n",
    "                        f\"Sampler: {meta.get('sampler', '')}, Version: {meta.get('Version', '')}\"\n",
    "                    )\n",
    "\n",
    "                    row = {\n",
    "                        'createdAt': item.get('createdAt', ''),\n",
    "                        'url': item.get('url', ''),\n",
    "                        'positivePrompt': meta.get('prompt', '').replace(',', '') if isinstance(meta, dict) else '',\n",
    "                        'negativePrompt': meta.get('negativePrompt', '') if isinstance(meta, dict) else '',\n",
    "                        'nsfw': item.get('nsfw', ''),\n",
    "                        'nsfwLevel': item.get('nsfwLevel', ''),\n",
    "                        'browsingLevel': item.get('browsingLevel', ''),\n",
    "                        'likeCount': stats.get('likeCount', 0),\n",
    "                        'dislikeCount': stats.get('dislikeCount', 0),\n",
    "                        'heartCount': stats.get('heartCount', 0),\n",
    "                        'cryCount': stats.get('cryCount', 0),\n",
    "                        'laughCount': stats.get('laughCount', 0),\n",
    "                        'commentCount': stats.get('commentCount', 0),\n",
    "                        'username': item.get('username', ''),\n",
    "                        'Model': meta.get('Model', ''),\n",
    "                        'Meta': meta_details,\n",
    "                        'VAE': meta.get('VAE', 'N/A')  # Dedicated column for VAE\n",
    "                    }\n",
    "\n",
    "                    # Handle up to six resources\n",
    "                    for i in range(6):\n",
    "                        if i < len(resources):\n",
    "                            resource = resources[i]\n",
    "                            resource_detail = (\n",
    "                                f\"Name: {resource.get('name', '')}, Type: {resource.get('type', '')}, Weight: {resource.get('weight', 'N/A')}\"\n",
    "                            )\n",
    "                            row[f'Resource{i+1}'] = resource_detail\n",
    "                        else:\n",
    "                            row[f'Resource{i+1}'] = None\n",
    "\n",
    "                    writer.writerow(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_output = current_dir / 'data/csv/preprocess_civiverse_social.csv'\n",
    "directory = current_dir / 'data/image-metadata/json-sorted'\n",
    "json_files = find_json_files(directory)\n",
    "write_to_csv(json_files, csv_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### anonymize usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymize_usernames(input_csv, anonymized_csv):\n",
    "    \"\"\"Anonymize usernames in the input CSV while keeping the rest of the data unchanged.\"\"\"\n",
    "    # Increase the maximum field size limit\n",
    "    csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "    with open(input_csv, 'r', encoding='utf-8') as infile, \\\n",
    "         open(anonymized_csv, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        \n",
    "        reader = csv.DictReader(infile)\n",
    "        fieldnames = reader.fieldnames  # Captures all existing column names\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in reader:\n",
    "            # Hash the username\n",
    "            if row['username']:  # Ensure there's a username to hash\n",
    "                hash_object = hashlib.sha256(row['username'].encode())\n",
    "                username_hash = hash_object.hexdigest()\n",
    "                row['username'] = username_hash  # Replace the username in the row with its hash\n",
    "            \n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = current_dir / 'data/csv/preprocess_civiverse_social.csv'\n",
    "anonymized_csv = current_dir / 'data/csv/preprocess_civiverse_social_an.csv'\n",
    "anonymize_usernames(input_csv, anonymized_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum-up reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv = current_dir / 'data/csv/Civiverse_social_scores.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = anonymized_csv  # Replace with the path to your file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'createdAt' to datetime, explicitly handling ISO8601 formats\n",
    "data['createdAt'] = pd.to_datetime(data['createdAt'], format='ISO8601', errors='coerce')\n",
    "\n",
    "# Define the cutoff date as timezone-aware\n",
    "cutoff_date = pd.Timestamp('2024-09-30', tz='UTC')\n",
    "\n",
    "# Calculate total social reactions\n",
    "data['socialReactions'] = data[['likeCount', 'dislikeCount', 'heartCount']].sum(axis=1)\n",
    "\n",
    "# Calculate days on the platform\n",
    "data['daysOnPlatform'] = (cutoff_date - data['createdAt']).dt.days\n",
    "\n",
    "# Sort by social reactions\n",
    "sorted_data = data.sort_values(by='socialReactions', ascending=False)\n",
    "\n",
    "# Save or display the processed data\n",
    "sorted_data.to_csv(output_csv, index=False)  # Replace with your desired save path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization and time penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = current_dir / 'data/csv/Civiverse_social_scores.csv'\n",
    "output_csv = current_dir / 'data/csv/Civiverse_normalized_social_scores.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   socialReactions  normalizedReactions  daysOnPlatform  timePenalty  \\\n",
      "0            11415             1.000000             178     0.056541   \n",
      "1            11413             0.999825             178     0.056541   \n",
      "2            10406             0.911577             142     0.069926   \n",
      "3            10406             0.911577             142     0.069926   \n",
      "4            10393             0.910437              94     0.097085   \n",
      "\n",
      "   socialEngagement  \n",
      "0          0.943459  \n",
      "1          0.943283  \n",
      "2          0.841651  \n",
      "3          0.841651  \n",
      "4          0.813353  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "file_path =  input_csv  # Replace with the path to your file\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure 'socialReactions' is numeric\n",
    "data['socialReactions'] = pd.to_numeric(data['socialReactions'], errors='coerce')\n",
    "\n",
    "# Normalize the social reactions (0-1 scale)\n",
    "scaler = MinMaxScaler()\n",
    "data['normalizedReactions'] = scaler.fit_transform(data[['socialReactions']])\n",
    "\n",
    "# Handle 'daysOnPlatform' to avoid zero or negative values\n",
    "offset = 1  # To control the penalty and avoid zero division\n",
    "data['daysOnPlatform'] = data['daysOnPlatform'].apply(lambda x: max(x, 1))\n",
    "\n",
    "# Calculate the time penalty\n",
    "data['timePenaltyRaw'] = 1 / (1 + np.log(data['daysOnPlatform'] + offset))\n",
    "\n",
    "# Normalize the time penalty (0-1 scale)\n",
    "data['timePenalty'] = scaler.fit_transform(data[['timePenaltyRaw']])\n",
    "\n",
    "# Adjust the social engagement score by subtracting the normalized time penalty\n",
    "data['socialEngagement'] = data['normalizedReactions'] - data['timePenalty']\n",
    "\n",
    "# Ensure no negative engagement scores (optional, if needed)\n",
    "data['socialEngagement'] = data['socialEngagement'].apply(lambda x: max(x, 0))\n",
    "\n",
    "# Display the first few rows\n",
    "print(data[['socialReactions', 'normalizedReactions', 'daysOnPlatform', 'timePenalty', 'socialEngagement']].head())\n",
    "\n",
    "data.to_csv(output_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
