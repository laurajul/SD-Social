{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image metadata scraping and CSV dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image metadata obtainment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## path thingy\n",
    "try: #scripts\n",
    "    current_dir = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # jupyter\n",
    "    current_dir = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define the input timestamp in ISO 8601 format\n",
    "input_timestamp = \"2024-09-30T06:25:02.809Z\"  # Replace with your desired timestamp\n",
    "\n",
    "# Function to convert an ISO 8601 date string to a Unix timestamp in milliseconds with a 2-hour offset\n",
    "def iso_to_timestamp(iso_str):\n",
    "    # Parse the ISO 8601 date string (including milliseconds and 'Z' indicating UTC)\n",
    "    date = datetime.strptime(iso_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    # Add 2 hours to the parsed date\n",
    "    adjusted_date = date + timedelta(hours=2)\n",
    "    # Convert to Unix timestamp (in milliseconds)\n",
    "    timestamp = int(adjusted_date.timestamp() * 1000)\n",
    "    return timestamp\n",
    "\n",
    "# Convert the input timestamp to an initial cursor\n",
    "initial_cursor = iso_to_timestamp(input_timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_images = 2000000000 # some huge number now redundant because API limit at 50'000\n",
    "\n",
    "def get_image_metadata():\n",
    "    base_url = \"https://civitai.com/api/v1/images\"\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer APITOKEN\"  # Replace with your actual API token\n",
    "    }\n",
    "    params = {\n",
    "        \"sort\": \"Most Reactions\",\n",
    "        \"nsfw\": \"Soft\",\n",
    "        \"cursor\": f\"0|{initial_cursor}\"\n",
    "    }\n",
    "\n",
    "    # Use pathlib to create the base directory for saving files\n",
    "    current_dir = Path.cwd()\n",
    "    base_directory_path = current_dir / f\"data/raw/001/{input_timestamp.replace(':', '').replace('T', '_').replace('.', '_').replace('Z', '')}\"\n",
    "    file_counter = 0\n",
    "\n",
    "    # Create a folder based on the input timestamp\n",
    "    base_directory_path.mkdir(parents=True, exist_ok=True)\n",
    "    sub_directory_path = base_directory_path\n",
    "\n",
    "    retry_delay = 300  # 300 seconds / 5 minutes\n",
    "    retries_without_cursor = 0  # Track consecutive retries without a new cursor\n",
    "\n",
    "    while True:\n",
    "        response = requests.get(base_url, headers=headers, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            items = data.get('items', [])\n",
    "            if not items:\n",
    "                print(\"No more data available.\")\n",
    "                retries_without_cursor += 1\n",
    "                if retries_without_cursor > 5:  # Allow up to 5 retries before stopping\n",
    "                    print(\"Reached the end of the data after multiple retries.\")\n",
    "                    break\n",
    "                time.sleep(retry_delay)\n",
    "                continue\n",
    "\n",
    "            retries_without_cursor = 0  # Reset if we get data\n",
    "\n",
    "            next_cursor = data.get('metadata', {}).get('nextCursor')\n",
    "            if next_cursor:\n",
    "                # Increment the cursor by 50 (e.g., \"0|1722470401000\" -> \"50|1722470401000\")\n",
    "                cursor_value = int(params['cursor'].split(\"|\")[0])\n",
    "                new_cursor_value = cursor_value + 50\n",
    "                params['cursor'] = f\"{new_cursor_value}|{params['cursor'].split('|')[1]}\"\n",
    "            else:\n",
    "                print(\"No new cursor returned, stopping.\")\n",
    "                break\n",
    "\n",
    "            file_counter += 1\n",
    "            if file_counter % max_images == 0:\n",
    "                time_stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                sub_directory_path = base_directory_path / f\"{input_timestamp.replace(':', '').replace('T', '_').replace('.', '_').replace('Z', '')}_session_{time_stamp}\"\n",
    "                sub_directory_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            file_path = sub_directory_path / f'most_recent_{file_counter}.json'\n",
    "            with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                json.dump(data, file, indent=4)\n",
    "\n",
    "        elif response.status_code == 502:\n",
    "            print(f\"Received HTTP 502. Retrying in {retry_delay // 60} minutes.\")\n",
    "            time.sleep(retry_delay)  # Wait for 5 minutes before retrying\n",
    "        else:\n",
    "            print(f\"Failed to fetch data: HTTP {response.status_code}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_image_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## json chronological data sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_files(source_dir, target_dir, max_items_per_file=100):\n",
    "    print(f\"Starting to organize files from {source_dir} to {target_dir}\")\n",
    "    item_buffer = []\n",
    "    file_count = 0\n",
    "\n",
    "    # Walk through all files in the source directory\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        if '.ipynb_checkpoints' in root:\n",
    "            continue  # Skip .ipynb_checkpoints directories\n",
    "        print(f\"Checking directory: {root}\")\n",
    "        for filename in files:\n",
    "            #print(f\"Found files: {files}\")\n",
    "            if filename.lower().endswith('.json'):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                try:\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        data = json.load(file)\n",
    "                        items = data.get('items', [])  # Get the list of items\n",
    "                        for item in items:\n",
    "                            item_buffer.append(item)\n",
    "                            # Write out the buffer if it has reached the maximum size\n",
    "                            if len(item_buffer) >= max_items_per_file:\n",
    "                                write_items(item_buffer[:max_items_per_file], target_dir)\n",
    "                                item_buffer = item_buffer[max_items_per_file:]\n",
    "                                file_count += 1\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Error decoding JSON from file {file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred with file {file_path}: {e}\")\n",
    "\n",
    "    # Write any remaining items in the buffer\n",
    "    if item_buffer:\n",
    "        write_items(item_buffer, target_dir)\n",
    "        file_count += 1\n",
    "\n",
    "    #print(f\"Processed {file_count} files.\")\n",
    "\n",
    "def write_items(items, target_dir):\n",
    "    # Use the createdAt from the first item to determine the directory\n",
    "    created_at = items[0].get('createdAt')\n",
    "    if created_at:\n",
    "        date_obj = datetime.fromisoformat(created_at.rstrip(\"Z\"))\n",
    "        new_dir = os.path.join(target_dir, f\"{date_obj.year}\", f\"{date_obj.year}-{date_obj.month:02d}\", f\"{date_obj.year}-{date_obj.month:02d}-{date_obj.day:02d}\")\n",
    "        os.makedirs(new_dir, exist_ok=True)\n",
    "        new_file_path = os.path.join(new_dir, f\"batch_{date_obj.strftime('%Y%m%dT%H%M%S')}.json\")\n",
    "        with open(new_file_path, 'w', encoding='utf-8') as new_file:\n",
    "            json.dump(items, new_file, indent=4)\n",
    "        #print(f\"Wrote {len(items)} items to {new_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to organize files from /home/lauhp/000_PHD/000_005_COURSES/Data_science/data/raw/001 to /home/lauhp/000_PHD/000_005_COURSES/Data_science/data/image-metadata/json-sorted\n",
      "Checking directory: /home/lauhp/000_PHD/000_005_COURSES/Data_science/data/raw/001\n",
      "Checking directory: /home/lauhp/000_PHD/000_005_COURSES/Data_science/data/raw/001/2024-09-30_062502_809\n"
     ]
    }
   ],
   "source": [
    "source_dir = current_dir / 'data/raw/001'\n",
    "target_dir = current_dir / 'data/image-metadata/json-sorted'\n",
    "\n",
    "organize_files(source_dir, target_dir, max_items_per_file=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV creation and user name hashingÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### walk through json files and write to csv functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: #scripts\n",
    "    current_dir = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # jupyter\n",
    "    current_dir = Path.cwd()\n",
    "\n",
    "def find_json_files(directory):\n",
    "    \"\"\"Walk through the directory and its subdirectories to find all JSON files.\"\"\"\n",
    "    json_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                json_files.append(os.path.join(root, file))\n",
    "    return json_files\n",
    "\n",
    "def write_to_csv(json_files, output_csv):\n",
    "    \"\"\"Read JSON files, extract data, and write to a CSV file.\"\"\"\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = [\n",
    "            'createdAt', 'url', 'positivePrompt', 'negativePrompt', 'nsfw', 'nsfwLevel',\n",
    "            'browsingLevel', 'likeCount', 'dislikeCount', 'heartCount', 'cryCount', \n",
    "            'laughCount', 'commentCount', 'username', 'Model', 'Meta', 'VAE'\n",
    "        ] + [f'Resource{i+1}' for i in range(6)]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for json_file in json_files:\n",
    "            with open(json_file, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                for item in data:\n",
    "                    meta = item.get('meta', {}) or {}\n",
    "                    stats = item.get('stats', {}) or {}\n",
    "                    resources = meta.get('resources', []) if isinstance(meta, dict) else []\n",
    "\n",
    "                    # Simplified Meta details without VAE\n",
    "                    meta_details = (\n",
    "                        f\"Size: {meta.get('Size', '')}, Seed: {meta.get('seed', '')}, Steps: {meta.get('steps', '')}, \"\n",
    "                        f\"Sampler: {meta.get('sampler', '')}, Version: {meta.get('Version', '')}\"\n",
    "                    )\n",
    "\n",
    "                    row = {\n",
    "                        'createdAt': item.get('createdAt', ''),\n",
    "                        'url': item.get('url', ''),\n",
    "                        'positivePrompt': meta.get('prompt', '').replace(',', '') if isinstance(meta, dict) else '',\n",
    "                        'negativePrompt': meta.get('negativePrompt', '') if isinstance(meta, dict) else '',\n",
    "                        'nsfw': item.get('nsfw', ''),\n",
    "                        'nsfwLevel': item.get('nsfwLevel', ''),\n",
    "                        'browsingLevel': item.get('browsingLevel', ''),\n",
    "                        'likeCount': stats.get('likeCount', 0),\n",
    "                        'dislikeCount': stats.get('dislikeCount', 0),\n",
    "                        'heartCount': stats.get('heartCount', 0),\n",
    "                        'cryCount': stats.get('cryCount', 0),\n",
    "                        'laughCount': stats.get('laughCount', 0),\n",
    "                        'commentCount': stats.get('commentCount', 0),\n",
    "                        'username': item.get('username', ''),\n",
    "                        'Model': meta.get('Model', ''),\n",
    "                        'Meta': meta_details,\n",
    "                        'VAE': meta.get('VAE', 'N/A')  # Dedicated column for VAE\n",
    "                    }\n",
    "\n",
    "                    # Handle up to six resources\n",
    "                    for i in range(6):\n",
    "                        if i < len(resources):\n",
    "                            resource = resources[i]\n",
    "                            resource_detail = (\n",
    "                                f\"Name: {resource.get('name', '')}, Type: {resource.get('type', '')}, Weight: {resource.get('weight', 'N/A')}\"\n",
    "                            )\n",
    "                            row[f'Resource{i+1}'] = resource_detail\n",
    "                        else:\n",
    "                            row[f'Resource{i+1}'] = None\n",
    "\n",
    "                    writer.writerow(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_output = current_dir / 'data/csv/preprocess_civiverse_social.csv'\n",
    "directory = current_dir / 'data/image-metadata/json-sorted'\n",
    "json_files = find_json_files(directory)\n",
    "write_to_csv(json_files, csv_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### anonymize usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymize_usernames(input_csv, anonymized_csv):\n",
    "    \"\"\"Anonymize usernames in the input CSV while keeping the rest of the data unchanged.\"\"\"\n",
    "    # Increase the maximum field size limit\n",
    "    csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "    with open(input_csv, 'r', encoding='utf-8') as infile, \\\n",
    "         open(anonymized_csv, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        \n",
    "        reader = csv.DictReader(infile)\n",
    "        fieldnames = reader.fieldnames  # Captures all existing column names\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in reader:\n",
    "            # Hash the username\n",
    "            if row['username']:  # Ensure there's a username to hash\n",
    "                hash_object = hashlib.sha256(row['username'].encode())\n",
    "                username_hash = hash_object.hexdigest()\n",
    "                row['username'] = username_hash  # Replace the username in the row with its hash\n",
    "            \n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = current_dir / 'data/csv/preprocess_civiverse_social.csv'\n",
    "anonymized_csv = current_dir / 'data/csv/preprocess_civiverse_social_an.csv'\n",
    "anonymize_usernames(input_csv, anonymized_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum-up reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv = current_dir / 'data/csv/Civiverse_social_scores.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = anonymized_csv  # Replace with the path to your file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'createdAt' to datetime, explicitly handling ISO8601 formats\n",
    "data['createdAt'] = pd.to_datetime(data['createdAt'], format='ISO8601', errors='coerce')\n",
    "\n",
    "# Define the cutoff date as timezone-aware\n",
    "cutoff_date = pd.Timestamp('2024-09-30', tz='UTC')\n",
    "\n",
    "# Calculate total social reactions\n",
    "data['socialReactions'] = data[['likeCount', 'dislikeCount', 'heartCount']].sum(axis=1)\n",
    "\n",
    "# Calculate days on the platform\n",
    "data['daysOnPlatform'] = (cutoff_date - data['createdAt']).dt.days\n",
    "\n",
    "# Sort by social reactions\n",
    "sorted_data = data.sort_values(by='socialReactions', ascending=False)\n",
    "\n",
    "# Save or display the processed data\n",
    "sorted_data.to_csv(output_csv, index=False)  # Replace with your desired save path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization and time penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = current_dir / 'data/csv/Civiverse_social_scores.csv'\n",
    "output_csv = current_dir / 'data/csv/Civiverse_normalized_social_scores.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   socialReactions  normalizedReactions  daysOnPlatform  timePenalty  \\\n",
      "0            11413             1.000000             178     0.056140   \n",
      "1            10406             0.906552             142     0.069530   \n",
      "2            10393             0.905345              94     0.096700   \n",
      "3             9603             0.832034             144     0.068673   \n",
      "4             9415             0.814588             145     0.068250   \n",
      "\n",
      "   socialEngagement  \n",
      "0          0.943860  \n",
      "1          0.837021  \n",
      "2          0.808645  \n",
      "3          0.763361  \n",
      "4          0.746338  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "file_path =  input_csv  # Replace with the path to your file\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure 'socialReactions' is numeric\n",
    "data['socialReactions'] = pd.to_numeric(data['socialReactions'], errors='coerce')\n",
    "\n",
    "# Normalize the social reactions (0-1 scale)\n",
    "scaler = MinMaxScaler()\n",
    "data['normalizedReactions'] = scaler.fit_transform(data[['socialReactions']])\n",
    "\n",
    "# Handle 'daysOnPlatform' to avoid zero or negative values\n",
    "offset = 1  # To control the penalty and avoid zero division\n",
    "data['daysOnPlatform'] = data['daysOnPlatform'].apply(lambda x: max(x, 1))\n",
    "\n",
    "# Calculate the time penalty\n",
    "data['timePenaltyRaw'] = 1 / (1 + np.log(data['daysOnPlatform'] + offset))\n",
    "\n",
    "# Normalize the time penalty (0-1 scale)\n",
    "data['timePenalty'] = scaler.fit_transform(data[['timePenaltyRaw']])\n",
    "\n",
    "# Adjust the social engagement score by subtracting the normalized time penalty\n",
    "data['socialEngagement'] = data['normalizedReactions'] - data['timePenalty']\n",
    "\n",
    "# Ensure no negative engagement scores (optional, if needed)\n",
    "data['socialEngagement'] = data['socialEngagement'].apply(lambda x: max(x, 0))\n",
    "\n",
    "# Display the first few rows\n",
    "print(data[['socialReactions', 'normalizedReactions', 'daysOnPlatform', 'timePenalty', 'socialEngagement']].head())\n",
    "\n",
    "data.to_csv(output_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
